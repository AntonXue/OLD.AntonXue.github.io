\section{Approximating Languages}
Given a measure space
\(\parens{\mcal{L}, \sigma\parens{L}, \lambda}\),
we may consider how similar two languages are.
For two languages \(L_1, L_2 \in \sigma\parens{\mcal{L}}\),
a ``natural'' difference is to consider their
symmetric set difference:
\begin{align*}
  d\parens{L_1, L_2}
    = \lambda\parens{L_1 \triangle L_2}
    = \lambda\parens{\parens{L_1 \setminus L_2} \cup \parens{L_2 \setminus L_1}}
\end{align*}
Recall that by the definition of a \(\sigma\)-algebra,
the symmetric set difference \(L_1 \triangle L_2\)
is in \(\sigma\parens{\mcal{L}}\), and therefore measurable.

We hope restrict our attention to regular languages for now,
or in other words, the class of languages precisely recognized by
DFAs, and ask the following:

\begin{question}
  Given a regular language \(L\) recognized by a minimal DFA \(A\)
  and some \(\varepsilon > 0\),
  does there exist a regular language \(L^\prime\) recognized by \(A^\prime\)
  such that \(A^\prime\) has less states than \(A\),
  and \(d\parens{L, L^\prime} < \varepsilon\)?
\end{question}


\begin{example}
  Consider \(\Sigma = \braces{a}\),
  where \(L_1 = aa^\star\) and \(L_2 = aaa^\star\),
  and assume a geometric probability measure for \(\eta\)
  with success of probability \(0 < p \leq 1\),
  through which \(\lambda\) is defined.

  Recall that for the geometric distribution where \(n \in \Zp\):
  \begin{align*}
    \eta\parens{n} = \parens{1 - p}^{n - 1} p
  \end{align*}
  Observe that for \(L_1\) and \(L_2\), we have:
  \begin{align*}
    L_1 &=
      \underbrace{\braces{}}_{\text{length \(0\)}} \cup
      \underbrace{\braces{a}}_{\text{length \(1\)}} \cup
      \underbrace{\braces{aa}}_{\text{length \(2\)}} \cup
      \underbrace{\braces{aaa}}_{\text{length \(3\)}} \cup
      \ldots \\
    L_2 &= 
      \underbrace{\braces{}}_{\text{length \(0\)}} \cup
      \underbrace{\braces{}}_{\text{length \(1\)}} \cup
      \underbrace{\braces{aa}}_{\text{length \(2\)}} \cup
      \underbrace{\braces{aaa}}_{\text{length \(3\)}} \cup
      \ldots
  \end{align*}
  In other words, the only set on which the two languages differ is
  strings of length \(1\), which \(L_1\) has, but \(L_2\) does not.
  For the difference, this then means that:
  \begin{align*}
    d\parens{L_1, L_2}
      = \lambda\parens{L_1 \triangle L_2}
      = \lambda\parens{\braces{a}}
      = \frac{\abs{\braces{a}}}{\abs{\Sigma^k}} \eta \parens{1}
      = \frac{1}{1} p = p
  \end{align*}
  In other words, with probability \(p\),
  we can distinguish random strings generated from \(L_1\) and \(L_2\),
  where the probability distribution is geometric,
  and over the length of the strings.
  Selection of the strings once length is fixed is irrelevant because
  each set corresponding to a length contains only one string.
\end{example}

Assume a (regular) language \(L\) given as an automata or regular
expression, whichever may be convenient, and measure \(\lambda_{n}\).
A few challenges lie ahead:

\begin{question}
  How can we quickly measure \(\lambda_{\eta}\parens{L}\)?
\end{question}

\begin{question}
  How can we quickly generate a word of some length from \(L\)?
\end{question}

The word ``quickly'' here is key:
many methods that can be thrown together to answer these questions
are intrinsically exponential,
so if we can introduce sub-exponential time techniques
that would be pretty cool.

\input{approx/counting.tex}

