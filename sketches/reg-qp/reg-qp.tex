\documentclass[12pt]{article}

% Packages
\usepackage[margin=5em]{geometry} % 1 cm = 2.84528 em
\usepackage[backend=bibtex]{biblatex}
\bibliography{sources}
% \nocite{*}

\usepackage{lipsum}

% Paragraphs
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% Includes
\input{axlib.tex}

% Author
\title{Regularized Equality-Constrained Quadratic Programming}
\author{Anton Xue and Nikolai Matni}
\date{\today}
\date{}

% Document
\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
We look at regularized approximations
of equality-constrained quadratic programming.
In particular, how much does the optimal solution deviate
when a regularizer is introduced?


\section{Background}
Stephen Boyd and Lieven Vandenberghe~\cite{boyd2004convex}.

\subsection{Equality Constrained Quadratic Programming}

A quadratic program with equality constraints~\cite{boyd2004convex} is
\begin{align}
  \text{minimize} &\quad \frac{1}{2} x^\top Q x
    \label{eqn:qp} \\
  \text{subject to} &\quad Ax = b
\end{align}
with variable in \(x \in \mbb{R}^n\),
where \(Q \succeq 0\) and the constraint \(A \in \mbb{R}^{m \times n}\) is,
for our purposes, fat and full rank.

The \(\ell^2\) regularized version for \(\lambda > 0\) looks like
\begin{align}
  \text{minimize} &\quad \frac{1}{2} x^\top Q x + \lambda x^\top x
    \label{eqn:reg-qp} \\
  \text{subject to} &\quad Ax = b
\end{align}

\subsection{Karush-Kuhn-Tucker Conditions}
The Lagrangian for \eqref{eqn:qp} is of form
\begin{align*}
  L(x, p) = \frac{1}{2} x^\top Q x + p^\top (Ax - b)
\end{align*}
for which the
optimality conditions~\cite{boyd2004convex} are
\begin{align*}
  Q x^\star + A^\top p^\star = b,
    \qquad Ax^\star = b
\end{align*}
or more compactly expressed:
\begin{align}
  \begin{bmatrix} Q & A^\top \\ A & 0 \end{bmatrix}
  \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} 0 \\ b \end{bmatrix}
    \label{eqn:kkt-qp}
\end{align}

On the other hand the Lagrangian of the
regularized problem \eqref{eqn:reg-qp} is
\begin{align*}
  L_\lambda (x, p) = \frac{1}{2} x^\top (Q + \lambda I) x + p^\top (Ax - b)
\end{align*}
which has the KKT conditions
\begin{align}
  \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
  \begin{bmatrix} x_\lambda ^\star \\ p_\lambda ^\star \end{bmatrix}
    = \begin{bmatrix} 0 \\ b \end{bmatrix}
    \label{eqn:kkt-reg-qp}
\end{align}

\subsection{Matrix Inversion}
Consider a symmetric matrix
\begin{align*}
  Q = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}
\end{align*}
when \(A \succ 0\), define
the Schur complement \(S = C - B^\top A^{-1} B\).
Then
\begin{align*}
  \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}^{-1}
    \begin{bmatrix}
      A^{-1} + A^{-1} B S^{-1} B^\top A^{-1} & - A^{-1} B S^{-1} \\
      - S^{-1} B^\top A^{-1} & S^{-1}
    \end{bmatrix}
\end{align*}


\section{Bounding Norms}
Noting that by linearity
\begin{align*}
  \begin{bmatrix} Q & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  + \begin{bmatrix} \lambda I & 0 \\ 0 & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} \lambda x^\star \\ b \end{bmatrix}
\end{align*}
and so through subtracting equations,
\begin{align*}
  \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix}
      x^\star - x_{\lambda} ^\star \\ p^\star - p_\lambda ^\star
    \end{bmatrix}
    &= \begin{bmatrix} \lambda x^\star \\ 0 \end{bmatrix}
\end{align*}
Given our assumptions on \(Q + \lambda I \succ 0\) and \(A\) is full rank.
For ease of notation, define \(\Lambda = Q + \lambda I\).
The Schur complement is then
\(S = - A \Lambda^{-1} A^\top\), and
\begin{align*}
  \begin{bmatrix}
    x^\star - x_\lambda ^\star \\ p^\star - p_\lambda ^\star \end{bmatrix}
  = \begin{bmatrix}
      \Lambda^{-1} + \Lambda^{-1} A^\top S^{-1} A \Lambda^{-1}
        & - \Lambda^{-1} A^\top S^{-1} \\
      - S^{-1} A \Lambda^{-1} & S^{-1}
    \end{bmatrix}
    \begin{bmatrix}
      \lambda x^\star \\ 0
    \end{bmatrix}
\end{align*}
In other words:
\begin{align*}
  x^\star - x_\lambda ^\star
    = \parens{\Lambda^{-1} -
      \Lambda^{-1} A^\top (A \Lambda^{-1} A^\top)^{-1} A \Lambda^{-1}}
      \lambda x^\star
\end{align*}
To simplify notation slightly, use \(\Gamma = \Lambda^{-1}\)
because it looks like an upside-down \(L\).
Then
\begin{align*}
  x^\star - x_\lambda ^\star
    = \parens{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma)}
      \lambda x^\star
\end{align*}

\begin{theorem}
  Without loss of generality, assume that
  \begin{align*}
    Q = \begin{bmatrix} q_1 & & \\ & \ddots & \\ & & q_n \end{bmatrix},
      \qquad q_1 \geq \cdots \geq q_n > 0.
  \end{align*}
  Then for \(\lambda \leq q_n\),
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \parens{\frac{\lambda}{q_n + \lambda}
              - \frac{\lambda}{q_1 + \lambda}} \norm{x^\star}
  \end{align*}
\end{theorem}
\begin{proof}
  First note that
  \begin{align*}
    \Lambda =
      \begin{bmatrix}
        q_1 + \lambda & & \\ & \ddots & \\ & & q_n + \lambda
      \end{bmatrix},
      \qquad
      \Gamma = \Lambda^{-1}
        = \begin{bmatrix}
            \frac{1}{q_1 + \lambda} & & \\
            & \ddots & \\
            & & \frac{1}{q_n + \lambda}
          \end{bmatrix}
  \end{align*}
  We seek the bound the RHS of the inequality
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \norm{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma}
          \cdot \lambda \norm{x^\star}
  \end{align*}
  The primary challenge here is
  correctly \textit{lower bounding} the
  \(\Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma\) term.
  For this we \textit{upper bound} the \(A \Gamma A^\top\) term.
  Letting
  \begin{align*}
    A = U \Sigma V^\top
      = U \begin{bmatrix} \Sigma_1 & 0 \end{bmatrix}
          \begin{bmatrix} V_1 ^\top \\ V_2 ^\top \end{bmatrix},
        \qquad \Sigma_1 =
        \begin{bmatrix} \sigma_1 & & \\ & \ddots & \\ & & \sigma_m \end{bmatrix}
  \end{align*}
  be an SVD of \(A\); an upper bound of \(A \Gamma A^\top\) is
  \begin{align*}
    A \Gamma A^\top
      = U \Sigma_1 V_1 ^\top
          \begin{bmatrix}
            \frac{1}{q_1 + \lambda} & & \\
            & \ddots & \\
            & & \frac{1}{q_n + \lambda}
          \end{bmatrix}
          V_1 \Sigma_1 U^\top
      \preceq \frac{1}{q_n + \lambda} U \Sigma_1 ^2 U^\top
  \end{align*}
  Consequently, a lower bound for
  \(\Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma\) is
  \begin{align*}
    \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma
      \succeq
        \Gamma V_1 \Sigma_1 U^\top
          \parens{\frac{1}{q_n + \lambda} U \Sigma_1 ^2 U^\top}^{-1}
          U \Sigma_1 V_1 ^\top \Gamma
      = \parens{q_n + \lambda}
          \Gamma V_1 V_1 ^\top \Gamma
      \succeq \frac{q_n + \lambda}{q_1 + \lambda} \Gamma
  \end{align*}
  Then using this,
  \begin{align*}
    \Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma
      \preceq \Gamma - \frac{q_n + \lambda}{q_1 + \lambda} \Gamma
      \preceq \parens{\frac{1}{q_n + \lambda} - \frac{1}{q_1 + \lambda}} I
  \end{align*}
  for which we derive the desired bound
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \norm{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma}
          \cdot \lambda \norm{x^\star}
      \leq \parens{\frac{\lambda}{q_n + \lambda}
                - \frac{\lambda}{q_1 + \lambda}}
              \norm{x^\star}
  \end{align*}

\end{proof}

\section{Optimal Value Deviation}
One challenge with merely bounding
\(\norm{x^\star - x_\lambda ^\star}\) is that
this norm difference may grow unbounded.
Consider an instance where the objective \(Q\) has a non-trivial kernel:
we may have
\begin{align*}
  \norm{\Pi_{\ker Q} x^\star} \gg
  \norm{\Pi_{\ker Q} x_\lambda ^\star}
\end{align*}
where \(\Pi_{\ker Q}\) is the projection onto \(\ker Q\);
this is possible because the regularized problems would force
\(\Pi_{\ker Q} x_\lambda ^\star\) to be close to the origin
rather than hiding far away from zero inside \(\ker Q\).
Instead, another way of examining sub-optimality is to consider the 
difference
\begin{align*}
  \frac{1}{2} (x_\lambda ^\star)^\top Q x_\lambda ^\star
    - \frac{1}{2} (x^\star)^\top Q x^\star
    > 0
\end{align*}
which we know holds because \(x_\lambda ^\star\) is no more optimal
than \(x^\star\).

\begin{theorem}[\cite{kovanic1979pseudoinverse}]
  \label{thm:kovanic}
  If \(X\) is an \(n \times q\) matrix contained in the column-space
  of an \(n \times n\) symmetrical matrix \(V\), then
  \begin{align*}
    (V + X X^\top)^\dagger
      = V^\dagger
        - V ^\dagger X (I + X^\top V^\dagger X)^{-1} X^\top V^\dagger
  \end{align*}
  where ``to be in the column-space'' means the same as
  \(V V^\dagger X = X\),
  and in a more general case
  \((I - V V^\dagger)X \neq 0\).
\end{theorem}

\begin{theorem}
  Define
  \begin{align*}
    Z = I - A ^\dagger A,
      \qquad \hat{x} = A^\dagger b
  \end{align*}
  so that solutions take form \(\hat{x} + Z v\),
  then a bound on the optimality gap is
  \begin{align*}
    \frac{1}{2} (x_\lambda ^\star)^\top Q x_\lambda ^\star
      - \frac{1}{2} (x^\star)^\top Q x^\star
      \leq O\parens{\frac{\lambda q_0}{q_0 - \lambda}}
  \end{align*}
  where \(q_0\) is the smallest non-zero eigenvalue of
  \(Q\) and \(\lambda < q_0\).
\end{theorem}
\begin{proof}
  Let \(\hat{x} = A^\dagger b\), and \(\Lambda = Q + \lambda I\), then
  \begin{align*}
    x^\star = \hat{x} - Z u,
      \qquad x_\lambda ^\star = \hat{x} - Z y
  \end{align*}
  where
  \begin{align*}
    Z = I - A^\dagger A,
      \qquad
      u = (Z^\top M Z)^{\dagger} Z^\top Q \hat{x},
      \qquad
      y = (Z^\top \Lambda Z)^{\dagger} Z^\top \Lambda \hat{x}
  \end{align*}
  which can be derived from examining the optimality conditions of the
  quadratic problem.
  However, because \(\hat{x} \in \ran A^\top = (\ker A)^\perp\),
  there is a further simplification as
  \(Z^\top \hat{x} = 0\):
  \begin{align*}
    y = (Z^\top \Lambda Z)^{\dagger} Z^\top (Q + \lambda I) \hat{x}
      = (Z^\top \Lambda Z)^{\dagger} Z^\top Q \hat{x}
  \end{align*}
  Comparing the objective values attained by \(x_\lambda ^\star\)
  and \(x^\star\), we have
  \begin{align*}
    &\frac{1}{2} (x_\lambda ^\star)^\top Q x_\lambda ^\star
      - \frac{1}{2} (x^\star)^\top Q x^\star \\
    &\quad= \frac{1}{2} (\hat{x} + Z y)^\top Q (\hat{x} + Z y)
          - \frac{1}{2} (\hat{x} + Z u)^\top Q (\hat{x} + Z u) \\
    &\quad=
      (\hat{x})^\top Q Z (y - u)
       + \frac{1}{2} y^\top Z^\top Q Z y
       - \frac{1}{2} u^\top Z^\top Q Z u \\
    &\quad=
      (\hat{x})^\top Q Z (y - u)
        + \frac{1}{2}\parens{\norm{Q^{1/2} Z y}^2 - \norm{Q^{1/2} Z u}^2} \\
    &\quad=
      (\hat{x})^\top Q Z (y - u)
        + \frac{1}{2}
          \bracks{\parens{\norm{Q^{1/2} Z u} + \norm{Q^{1/2} Z y}}
                \parens{\norm{Q^{1/2} Z y} - \norm{Q^{1/2} Z u}}} \\
    &\quad\leq
      (\hat{x})^\top Q Z (y - u)
        + \frac{1}{2}
          \bracks{\parens{\norm{Q^{1/2} Z u} + \norm{Q^{1/2} Z y}}
            \cdot \norm{Q^{1/2} Z (y - u)}}
            \tag{Reverse triangle}
  \end{align*}
  Which simplifies to
  \begin{align}
    \frac{1}{2} (x_\lambda ^\star)^\top Q x_\lambda ^\star
      - \frac{1}{2} (x^\star)^\top Q x^\star
    \leq
      \parens{\norm{Q^{1/2} \hat{x}}
        + \frac{1}{2} \norm{Q^{1/2} Z u}
        + \frac{1}{2} \norm{Q^{1/2} Z y}}
            \cdot \norm{Q^{1/2} Z (y - u)}
      \label{eqn:qhalf-terms}
  \end{align}
  We now apply to Theorem~\ref{thm:kovanic}
  to \(Q^{1/2} Z y\):
  identifying the mapping
  \(V \mapsto Z^\top Q Z\) and \(X \mapsto \sqrt{\lambda} Z\),
  first show that \(X\) lies in the column space of \(V\).
  For this, consider the eigen decomposition
  \begin{align*}
    V = F D F^\top,
      \qquad F = \begin{bmatrix} F_1 & F_2 \end{bmatrix},
      \qquad D = \diag(d_1, \ldots, d_k, 0, \ldots, 0),
      \qquad X = \sqrt{\lambda} F_1
  \end{align*}
  where \(F\) is orthogonal.
  Then \(D^\dagger = \diag(d_1 ^{-1}, \ldots, d_k ^{-1}, 0, \ldots, 0)\)
  and so
  \begin{align*}
    V V^\dagger X
      = (F D F^\top)(F D^\dagger F^\top) \sqrt{\lambda} F_1
      = \sqrt{\lambda} F_1
      = X
  \end{align*}
  thus, Theorem~\ref{thm:kovanic} applies and
  \begin{align*}
    Q^{1/2} Z y
      &= Q^{1/2} Z \parens{Z ^\top (Q+\lambda I) Z}^\dagger Z^\top Q \hat{x} \\
      &= Q^{1/2} Z
          \bracks{V^\dagger
            - V^\dagger X (I + X^\top V^\dagger X)^{-1} X^\top V^\dagger}
            Z^\top Q \hat{x}
  \end{align*}
  With this, we now bound the terms that appear in \eqref{eqn:qhalf-terms}.
  For the easy one:
  \begin{align}
    \norm{Q^{1/2} \hat{x}}
      = \norm{Q^{1/2} A^\dagger b}
      \leq \lambda_{\max} (Q) ^{1/2}
          \frac{1}{\sigma_{\min} (A)}
          \norm{b}
      \label{eqn:qhalf-left}
  \end{align}
  For the inside second term, accounting for the
  fact that \(Z\) is a projection,
  \begin{align}
    \norm{Q^{1/2} Z u}
      = \norm{Q^{1/2} Z V^\dagger Z^\top Q A ^\dagger b}
      \leq
        \lambda_{\max} (Q) ^{3/2}
          \frac{1}{q_0}
          \frac{1}{\sigma_{\min} (A)}
          \norm{b}
      \label{eqn:qhalf-mid}
  \end{align}
  where \(q_0\) is the smallest non-zero eigenvalue of \(Q\).
  For the third inside term:
  \begin{align*}
    &\norm{Q^{1/2} Z y} \\
      &\quad= \norm{Q^{1/2} Z
          \bracks{V^\dagger
            - V^\dagger X(I + X^\top V^\dagger X)^{-1} X^\top V^\dagger}
          Z^\top Q A^\dagger b} \\
      &\quad\leq \norm{Q^{1/2} Z V ^\dagger Z^\top Q A^\dagger b}
        + \norm{Q^{1/2} Z V^\dagger X
          (I + X^\top V^\dagger X)^{-1} X^\top V^\dagger
          Z^\top Q A^\dagger b} \\
      &\quad\leq \norm{Q^{1/2} Z V ^\dagger Z^\top Q A^\dagger b}
        + \norm{Q}^{3/2} \norm{V^\dagger}^2 \cdot \norm{X}^2
          \cdot \norm{A^\dagger b}
          \cdot \norm{(I + X^\top V^\dagger X)^{-1}} \\
      &\quad\leq
        \frac{\lambda_{\max} (Q)^{3/2} \cdot \norm{b}}{q_0 \sigma_{\min} (A)}
        + \frac{\lambda_{\max} (Q)^{3/2} \cdot \norm{b} \lambda}
                {q_0 ^2 \sigma_{\min} (A)}
        \cdot \norm{(I + X^\top V^\dagger X)^{-1}} \\
      &\quad\leq
        \frac{\lambda_{\max} (Q)^{3/2} \cdot \norm{b}}{q_0 \sigma_{\min} (A)}
        + \frac{\lambda_{\max} (Q)^{3/2} \cdot \norm{b} \lambda}
                {q_0 ^2 \sigma_{\min} (A)}
          \sum_{l = 0}^{\infty} \norm{X^\top V^\dagger X}^l \\
      &\quad\leq
        \frac{\lambda_{\max} (Q)^{3/2} \cdot \norm{b}}{q_0 \sigma_{\min} (A)}
        + \frac{\lambda_{\max} (Q)^{3/2} \cdot \norm{b} \lambda}
                {q_0 ^2 \sigma_{\min} (A)}
          \sum_{l = 0}^{\infty} \parens{\lambda / q_0}^l \\
      &\quad\leq
        \frac{\lambda_{\max} (Q)^{3/2} \cdot \norm{b}}{q_0 \sigma_{\min} (A)}
        + \frac{\lambda_{\max} (Q)^{3/2} \cdot \norm{b} \lambda}
                {q_0 ^2 \sigma_{\min} (A)}
            \parens{1 - \frac{\lambda}{q_0}}^{-1}
  \end{align*}
  which requires \(\lambda < q_0\) for convergence.
  Finally for the right outer term:
  \begin{align*}
    \norm{Q^{1/2} Z (y - u)}
      = \norm{Q^{1/2} Z V^\dagger X
        (I + X^\top V^\dagger X)^{-1} X^\top V^\dagger Z^\top Q A ^\dagger b}
      \leq \frac{\lambda_{\max} (Q)^{3/2} \norm{b} \lambda}{q_0 ^2 \sigma_{\min} (A)}
      \parens{1 - \frac{\lambda}{q_0}}^{-1}
  \end{align*}
  Putting these together, we find that
  \begin{align*}
    &\frac{1}{2} \parens{x_\lambda ^\star}^\top Q x_\lambda ^\star
      - \frac{1}{2} x^\star Q x^\star \\
    &\quad\leq
      \parens{\norm{Q^{1/2} \hat{x}}
        + \frac{1}{2} \norm{Q^{1/2} Z u}
        + \frac{1}{2} \norm{Q^{1/2} Z y}}
            \cdot \norm{Q^{1/2} Z (y - u)} \\
    &\quad \leq
      \max\{\lambda_{\max} (Q)^2, \lambda_{\max} (Q)^3\}
       \cdot \max\{q_0 ^{-2}, q_0 ^{-3}, q_0 ^{-4}\}
       \cdot \parens{\frac{\norm{b}}{\sigma_{\min} (A)}}^2
       O \parens{\frac{\lambda q_0}{q_0 - \lambda}}
  \end{align*}

\end{proof}

\section{Interior Point Inspired}

The analysis of interior point methods, in particular log-barrier functions
in~\ref{boyd2004convex} Section 11.2
is gives yet another way to bound optimality gap.

Consider an optimization problem of form
\begin{align}
  \text{minimize}&\quad f(x) + \frac{1}{t} \phi(x) \label{eqn:fake-barrier} \\
  \text{subject to}&\quad Ax = b \nonumber
\end{align}
where
\begin{align*}
  f(x) = \frac{1}{2} x^\top Q x,
    \qquad \phi(x) = \norm{x}^2 - M
\end{align*}
where \(M > 0\) is a large constant.
This problem is equivalent to \(\ell^2\) regularized constrained
quadratic minimization,
and in fact that \(M\) constant is not needed, but nevertheless
this allows us to achieve theoretical guarantees.
Intuitively, as \(t \to \infty\),
the solution \(x^\star (t)\) converges to the optimal \(x^\star\).

\begin{theorem}
  Let \(p^\star\) be the optimal value for the
  original (quadratic linear equality constrained)
  problem and \(x^\star (t)\) be the optimal (central-path) solutions to
  \eqref{eqn:fake-barrier} as a function of \(t\), then
  the optimality gap is
  \begin{align*}
    f(x^\star (t)) - p^\star \leq \frac{M - \norm{x^\star (t))}}{t},
      \qquad M = \kappa(Q + (1/t)I) ^2 \cdot \norm{A^\dagger b}^2
              + \varepsilon
  \end{align*}
  for small \(\varepsilon > 0\).
\end{theorem}
\begin{proof}
  Based on the formulation of \eqref{eqn:fake-barrier},
  the optimality conditions are
  \begin{align*}
    \nabla f(x) + \frac{1}{t} \nabla \phi(x) + A^\top \nu = 0,
      \qquad
      Ax = b
  \end{align*}
  which we identify with the Lagrangian of the following problem:
  \begin{align}
    \text{minimize} \quad f(x),
      \qquad \text{subject to} \quad Ax = b, \quad \norm{x}^2 \leq M
      \label{eqn:barrier-problem}
  \end{align}
  with Lagrangian
  \begin{align*}
    L(x, \lambda, \nu)
      = f(x) + \lambda \parens{\norm{x}^2 - M} + \nu^\top (Ax - b)
  \end{align*}
  which suggests the mapping \(\lambda \mapsto 1/t\).
  
  Strong duality holds for \eqref{eqn:barrier-problem}
  since the problem is strictly feasible.
  This is because \(\hat{x} = A^\dagger b\) is the least-norm solution
  of \(Ax = b\),
  and examining the ellipsoidal level sets of
  \(x \mapsto x^\top (Q + (1/t) I) x\):
  major axis at \(\hat{x}\) has radius no more than
  \(\norm{\hat{x}} \cdot \kappa (Q + (1/t) I)\).
  As the optimal solution \(x^\star (t)\) must be at a \textit{smaller}
  level set (given this is a minimization problem),
  it likewise has a norm strictly smaller than
  \(\norm{\hat{x}} \cdot \kappa (Q + (1/t) I) + \delta\)
  for any small perturbation \(\delta > 0\).
  
  Because strong duality holds, letting \(p^\star\) be the optimal solution
  to \eqref{eqn:fake-barrier} and \(g(\lambda, \nu)\)
  the Lagrange dual function
  of \eqref{eqn:barrier-problem},
  \begin{align*}
    f(x^\star (t), \lambda, \nu^\star)
      = f(x^\star(t)) + \lambda (\norm{x}^2 - M)
      &\leq g(\lambda, \nu^\star) \leq p^\star
  \end{align*}
  and taking \(\lambda \mapsto 1 / t\) this simplifies to the desired result
  \begin{align*}
    f(x^\star (t)) - p^\star
      \leq \frac{M - \norm{x^\star (t)}^2}{t}
  \end{align*}
  

\end{proof}

\printbibliography

\end{document}

