\documentclass[12pt]{article}

% Packages
\usepackage[margin=5em]{geometry} % 1 cm = 2.84528 em
\usepackage[backend=bibtex]{biblatex}
\bibliography{sources}
% \nocite{*}

\usepackage{lipsum}

% Paragraphs
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% Includes
\input{axlib.tex}

% Author
\title{}
\author{Anton Xue and Nikolai Matni}
\date{\today}
\date{}

% Document
\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
We look at regularized approximations
of equality-constrained quadratic programming.
In particular, how much does the optimal solution deviate
when a regularizer is introduced?


\section{Background}
Stephen Boyd and Lieven Vandenberghe~\cite{boyd2004convex}.

\subsection{Equality Constrained Quadratic Programming}

A quadratic program with equality constraints~\cite{boyd2004convex} is
\begin{align}
  \text{minimize} &\quad \frac{1}{2} x^\top Q x
    \label{eqn:qp} \\
  \text{subject to} &\quad Ax = b
\end{align}
with variable in \(x \in \mbb{R}^n\),
where \(Q \succeq 0\) and the constraint \(A \in \mbb{R}^{m \times n}\) is,
for our purposes, fat and full rank.

The \(\ell^2\) regularized version for \(\lambda > 0\) looks like
\begin{align}
  \text{minimize} &\quad \frac{1}{2} x^\top Q x + \lambda x^\top x
    \label{eqn:reg-qp} \\
  \text{subject to} &\quad Ax = b
\end{align}

\subsection{Karush-Kuhn-Tucker Conditions}
The Lagrangian for \eqref{eqn:qp} is of form
\begin{align*}
  L(x, p) = \frac{1}{2} x^\top Q x + p^\top (Ax - b)
\end{align*}
for which the
optimality conditions~\cite{boyd2004convex} are
\begin{align*}
  Q x^\star + A^\top p^\star = b,
    \qquad Ax^\star = b
\end{align*}
or more compcatctly expressed:
\begin{align}
  \begin{bmatrix} Q & A^\top \\ A & 0 \end{bmatrix}
  \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} 0 \\ b \end{bmatrix}
    \label{eqn:kkt-qp}
\end{align}

On the other hand the Lagrangian of the
regularized problem \eqref{eqn:reg-qp} is
\begin{align*}
  L_\lambda (x, p) = \frac{1}{2} x^\top (Q + \lambda I) x + p^\top (Ax - b)
\end{align*}
which has the KKT conditions
\begin{align}
  \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
  \begin{bmatrix} x_\lambda ^\star \\ p_\lambda ^\star \end{bmatrix}
    = \begin{bmatrix} 0 \\ b \end{bmatrix}
    \label{eqn:kkt-reg-qp}
\end{align}

\subsection{Matrix Inversion}
Consider a symmetric matrix
\begin{align*}
  Q = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}
\end{align*}
when \(A \succ 0\), define
the Schur complement \(S = C - B^\top A^{-1} B\).
Then
\begin{align*}
  \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}^{-1}
    \begin{bmatrix}
      A^{-1} + A^{-1} B S^{-1} B^\top A^{-1} & - A^{-1} B S^{-1} \\
      - S^{-1} B^\top A^{-1} & S^{-1}
    \end{bmatrix}
\end{align*}


\section{Bounding Norms}
Noting that by linearity
\begin{align*}
  \begin{bmatrix} Q & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  + \begin{bmatrix} \lambda I & 0 \\ 0 & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} \lambda x^\star \\ b \end{bmatrix}
\end{align*}
and so through subtracting equations,
\begin{align*}
  \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix}
      x^\star - x_{\lambda} ^\star \\ p^\star - p_\lambda ^\star
    \end{bmatrix}
    &= \begin{bmatrix} \lambda x^\star \\ 0 \end{bmatrix}
\end{align*}
Given our assumptions on \(Q + \lambda I \succ 0\) and \(A\) is full rank.
For ease of notation, define \(\Lambda = Q + \lambda I\).
The Schur complement is then
\(S = - A \Lambda^{-1} A^\top\), and
\begin{align*}
  \begin{bmatrix}
    x^\star - x_\lambda ^\star \\ p^\star - p_\lambda ^\star \end{bmatrix}
  = \begin{bmatrix}
      \Lambda^{-1} + \Lambda^{-1} A^\top S^{-1} A \Lambda^{-1}
        & - \Lambda^{-1} A^\top S^{-1} \\
      - S^{-1} A \Lambda^{-1} & S^{-1}
    \end{bmatrix}
    \begin{bmatrix}
      \lambda x^\star \\ 0
    \end{bmatrix}
\end{align*}
In other words:
\begin{align*}
  x^\star - x_\lambda ^\star
    = \parens{\Lambda^{-1} -
      \Lambda^{-1} A^\top (A \Lambda^{-1} A^\top)^{-1} A \Lambda^{-1}}
      \lambda x^\star
\end{align*}
To simplify notation slightly, use \(\Gamma = \Lambda^{-1}\)
because it looks like an upside-down \(L\).
Then
\begin{align*}
  x^\star - x_\lambda ^\star
    = \parens{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma)}
      \lambda x^\star
\end{align*}

\begin{theorem}
  Without loss of generality, assume that
  \begin{align*}
    Q = \begin{bmatrix} q_1 & & \\ & \ddots & \\ & & q_n \end{bmatrix},
      \qquad q_1 \geq \cdots \geq q_n > 0.
  \end{align*}
  Then for \(\lambda \leq q_n\),
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \parens{\frac{\lambda}{q_n + \lambda}
              - \frac{\lambda}{q_1 + \lambda}} \norm{x^\star}
  \end{align*}
\end{theorem}
\begin{proof}
  First note that
  \begin{align*}
    \Lambda =
      \begin{bmatrix}
        q_1 + \lambda & & \\ & \ddots & \\ & & q_n + \lambda
      \end{bmatrix},
      \qquad
      \Gamma = \Lambda^{-1}
        = \begin{bmatrix}
            \frac{1}{q_1 + \lambda} & & \\
            & \ddots & \\
            & & \frac{1}{q_n + \lambda}
          \end{bmatrix}
  \end{align*}
  We seek the bound the RHS of the inequality
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \norm{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma}
          \cdot \lambda \norm{x^\star}
  \end{align*}
  The primary challenge here is
  correctly \textit{lower bounding} the
  \(\Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma\) term.
  For this we \textit{upper bound} the \(A \Gamma A^\top\) term.
  Letting
  \begin{align*}
    A = U \Sigma V^\top
      = U \begin{bmatrix} \Sigma_1 & 0 \end{bmatrix}
          \begin{bmatrix} V_1 ^\top \\ V_2 ^\top \end{bmatrix},
        \qquad \Sigma_1 =
        \begin{bmatrix} \sigma_1 & & \\ & \ddots & \\ & & \sigma_m \end{bmatrix}
  \end{align*}
  be an SVD of \(A\); an upper bound of \(A \Gamma A^\top\) is
  \begin{align*}
    A \Gamma A^\top
      = U \Sigma_1 V_1 ^\top
          \begin{bmatrix}
            \frac{1}{q_1 + \lambda} & & \\
            & \ddots & \\
            & & \frac{1}{q_n + \lambda}
          \end{bmatrix}
          V_1 \Sigma_1 U^\top
      \preceq \frac{1}{q_n + \lambda} U \Sigma_1 ^2 U^\top
  \end{align*}
  Consequently, a lower bound for
  \(\Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma\) is
  \begin{align*}
    \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma
      \succeq
        \Gamma V_1 \Sigma_1 U^\top
          \parens{\frac{1}{q_n + \lambda} U \Sigma_1 ^2 U^\top}^{-1}
          U \Sigma_1 V_1 ^\top \Gamma
      = \parens{q_n + \lambda}
          \Gamma V_1 V_1 ^\top \Gamma
      \succeq \frac{q_n + \lambda}{q_1 + \lambda} \Gamma
  \end{align*}
  Then using this,
  \begin{align*}
    \Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma
      \preceq \Gamma - \frac{q_n + \lambda}{q_1 + \lambda} \Gamma
      \preceq \parens{\frac{1}{q_n + \lambda} - \frac{1}{q_1 + \lambda}} I
  \end{align*}
  for which we derive the desired bound
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \norm{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma}
          \cdot \lambda \norm{x^\star}
      \leq \parens{\frac{\lambda}{q_n + \lambda}
                - \frac{\lambda}{q_1 + \lambda}}
              \norm{x^\star}
  \end{align*}

\end{proof}

\section{Optimal Value Deviation}
One challenge with merely bounding
\(\norm{x^\star - x_\lambda ^\star}\) is that
this norm difference may grow unbounded.
Consider an instance where the objective \(Q\) has a non-trivial kernel:
we may have
\begin{align*}
  \norm{\Pi_{\ker Q} x^\star} \gg
  \norm{\Pi_{\ker Q} x_\lambda ^\star}
\end{align*}
where \(\Pi_{\ker Q}\) is the projection onto \(\ker Q\);
this is possible because the regularized problems would force
\(\Pi_{\ker Q} x_\lambda ^\star\) to be close to the origin.
Instead, another way of examining sub-optimality is to consider the 
difference
\begin{align*}
  \frac{1}{2} (x_\lambda ^\star)^\top Q x_\lambda ^\star
    - \frac{1}{2} (x^\star)^\top Q x^\star
    > 0
\end{align*}
which we know holds because \(x_\lambda ^\star\) is no more optimal
than \(x^\star\).

\begin{theorem}[\cite{kovanic1979pseudoinverse}]
  \label{thm:kovanic}
  If \(X\) is an \(n \times q\) matirx contained in the column-space
  of an \(n \times n\) symmetrical matrix \(V\), then
  \begin{align*}
    (V + X X^\top)^\dagger
      = V^\dagger
        - V ^\dagger X (I + X^\top V^\dagger X)^{-1} X^\top V^\dagger
  \end{align*}
  where ``to be in the column-space'' means the same as
  \(V V^\dagger X = X\),
  and in a more general case
  \((I - V V^\dagger)X \neq 0\).
\end{theorem}

\begin{theorem}
  Define
  \begin{align*}
    Z = I - A ^\dagger A,
      \qquad \hat{x} = A^\dagger b
  \end{align*}
  so that solutions take form \(\hat{x} + Z v\),
  then a bound on the optimality gap is
  \begin{align*}
    &\frac{1}{2} (x_\lambda ^\star)^\top Q x_\lambda ^\star
      - \frac{1}{2} (x^\star)^\top Q x^\star \\
    &\qquad \leq
      \norm{Q^{1/2} \hat{x}}^2
        \cdot \norm{Q^{1/2} Z}^2
        \cdot \frac{\lambda m_0}{1 - \lambda m_0}
        + \norm{Q^{1/2} \hat{x}}^4
        \cdot \norm{Q^{1/2} Z}^4
        \cdot \parens{\frac{\lambda m_0}{1 - \lambda m_0}}^2
  \end{align*}
  where \(m_0\) is the smallest non-zero eigenvalue of
  \(V\).
\end{theorem}
\begin{proof}
  Let \(\hat{x} = A^\dagger b\), and \(\Lambda = Q + \lambda I\), then
  \begin{align*}
    x^\star = \hat{x} - Z u,
      \qquad x^\star = \hat{x} - Z y
  \end{align*}
  where
  \begin{align*}
    Z = I - A^\dagger A,
      \qquad
      u = (Z^\top M Z)^{\dagger} Z^\top Q \hat{x},
      \qquad
      y = (Z^\top \Lambda Z)^{\dagger} Z^\top \Lambda \hat{x}
  \end{align*}
  which can be derived from examining the optimality conditions of the
  quadratic problem.
  However, because \(\hat{x} \in \ker A^\top\),
  there is a further simplification:
  \begin{align*}
    y = (Z^\top \Lambda Z)^{\dagger} Z^\top (Q + \lambda I) \hat{x}
      = (Z^\top \Lambda Z)^{\dagger} Z^\top Q \hat{x}
  \end{align*}
  Comparing the objective values attained by \(x_\lambda ^\star\)
  and \(x^\star\), we have
  \begin{align*}
    &\frac{1}{2} (x_\lambda ^\star)^\top Q x_\lambda ^\star
      - \frac{1}{2} (x^\star)^\top Q x^\star \\
    &\quad= \frac{1}{2} (\hat{x} + Z y)^\top Q (\hat{x} + Z y)
          - \frac{1}{2} (\hat{x} + Z u)^\top Q (\hat{x} + Z u) \\
    &\quad=
      (\hat{x})^\top Q Z (y - u)
       + \frac{1}{2} y^\top Z^\top Q Z y
       - \frac{1}{2} u^\top Z^\top Q Z u \\
    &\quad=
      (\hat{x})^\top Q Z (y - u)
        + \frac{1}{2}\parens{\norm{Q^{1/2} Z y} - \norm{Q^{1/2} Z u}} \\
    &\quad\leq
      (\hat{x})^\top Q Z (y - u)
        + \frac{1}{2} \norm{Q^{1/2} Z (y - u)}^2
        \tag{Reverse triangle inequality} \\
    &\quad\leq
      \norm{Q^{1/2}\hat{x}} \cdot \norm{Q^{1/2} Z (y - u)}
        + \frac{1}{2} \norm{Q^{1/2} Z (y - u)}^2
  \end{align*}
  From here,
  \begin{align*}
    \norm{Q^{1/2} Z (y - u)}
      &= \norm{Q^{1/2} Z \bracks{
          \parens{Z^\top Q Z}^{\dagger} Z^\top Q \hat{x}
          - \parens{Z^\top \Lambda Z}^{\dagger} Z^\top Q \hat{x}}} \\
      &= \norm{Q^{1/2} Z \bracks{
          \parens{Z^\top Q Z}^{\dagger} Z^\top Q \hat{x}
          - \parens{Z^\top (Q + \lambda I) Z}^{\dagger} Z^\top Q \hat{x}}} \\
  \end{align*}
  We now apply Theorem~\ref{thm:kovanic}:
  identifying the mapping \(V \mapsto Z^\top Q Z\)
  and \(X \mapsto \sqrt{\lambda} Z^\top\).
  First we show that \(X\) lies in the column space of \(V\).
  Consider the eigenvalue decomposition
  \begin{align*}
    V = F^\top D F,
      \qquad D = \diag(d_1, \ldots, d_k, 0, \ldots, 0),
      \qquad X = \sqrt{\lambda} F^\top I,
  \end{align*}
  where \(F\) is orthogonal and \(D\) is diagonal.
  Then \(D^\dagger = \diag(d_1 ^{-1}, \ldots, d_k ^{-1}, 0, \ldots 0)\)
  and so
  \begin{align*}
    V V^\dagger X
      = (F^\top D F) (F^\top D^\dagger F) F^\top \sqrt{\lambda} I
      = \sqrt{\lambda} F^\top
      = X
  \end{align*}
  Now, the above norm-bound can be re-written as follows,
  \begin{align*}
    &\norm{Q^{1/2} Z (y - u)} \\
      &\quad =
        \norm{Q^{1/2} Z
          \bracks{
            V^\dagger
              - \parens{V^\dagger
                - V^\dagger X(I + X^\top V^\dagger X)^{-1} X^\top V^\dagger}}
            Z^\top Q \hat{x}} \\
      &\quad =
        \norm{Q^{1/2} Z
          \parens{V^\dagger X(I+X^\top V^\dagger X)^{-1} X^\top V^\dagger}
            Z^\top Q \hat{x}} \\
      &\quad \leq
        \norm{Q^{1/2} Z}
          \cdot \norm{V^\dagger X}^2
          \cdot \norm{Z^\top Q \hat{x}}
          \cdot \norm{(I + X^\top V^\dagger X)^{-1}} \\
      &\quad \leq
        \norm{Q^{1/2} Z}
          \cdot \norm{V^\dagger X}^2
          \cdot \norm{Z^\top Q \hat{x}}
          \cdot \parens{\sum_{l = 0}^{\infty} \norm{X^\top V^\dagger X}^l} \\
      &\quad \leq
        \norm{Q^{1/2} Z}
          \cdot \norm{V^\dagger X}^2
          \cdot \norm{Z^\top Q \hat{x}}
          \cdot \parens{\sum_{l = 0}^{\infty} \lambda^l \norm{V^\dagger}^l} \\
      &\quad \leq
        \norm{Q^{1/2} Z}
          \cdot \norm{V^\dagger X}^2
          \cdot \norm{Z^\top Q \hat{x}}
          \cdot \parens{1 - \lambda m_0}^{-1} \\
      &\quad \leq
        \norm{Q^{1/2} Z}
          \cdot \norm{Z^\top Q \hat{x}}
          \cdot \frac{\lambda m_0}{1 - \lambda m_0} \\
      &\quad \leq \norm{Q^{1/2} Z}^2 \cdot \norm{Q^{1/2} \hat{x}}
          \cdot \frac{\lambda m_0}{1 - \lambda m_0}
  \end{align*}
  where \(m_0\) is the smallest non-zero eigenvalue of \(V\),
  and so
  \begin{align*}
    &\frac{1}{2} (x_\lambda ^\star)^\top Q x_\lambda ^\star
      - \frac{1}{2} (x^\star)^\top Q x^\star \\
    &\quad \leq \norm{Q^{1/2} \hat{x}}
          \cdot \norm{Q^{1/2} Z (y - u)}
          + \frac{1}{2} \norm{Q^{1/2} Z (y - u)}^2 \\
    &\quad \leq
      \norm{Q^{1/2} \hat{x}}^2
        \cdot \norm{Q^{1/2} Z}^2
        \cdot \frac{\lambda m_0}{1 - \lambda m_0}
        + \norm{Q^{1/2} \hat{x}}^4
        \cdot \norm{Q^{1/2} Z}^4
        \cdot \parens{\frac{\lambda m_0}{1 - \lambda m_0}}^2
  \end{align*}
  and this vanishes as \(\lambda \to 0\).

\end{proof}



\printbibliography

\end{document}

