\documentclass[12pt]{article}

% Packages
\usepackage[margin=5em]{geometry} % 1 cm = 2.84528 em
\usepackage[backend=bibtex]{biblatex}
\usepackage{xcolor}
\bibliography{sources}
% \nocite{*}

\usepackage{lipsum}

% Paragraphs
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% Includes
\input{antonxue-lib.tex}
\input{ker-lib.tex}


% Author
\title{Kernel Methods and Automata}
\author{Anton Xue}
% \date{\today}
\date{}

% Document
\begin{document}
\maketitle

\section{Introduction}
In this sketch we study the relation between graph kernels and
finite state machines.
We study previous work in algebraic formulation for automata theory
and kernel methods, with an interest in how these techniques
may be extended to formal methods.



\section{Preliminaries}


\subsection{Algebraic Automata Theory}
Previous work in formulation in algebraic foundations for automata
theory exist in literature~\cite{kuich2012semirings},
and much of our notation is taken from~\cite{cortes2004rational}.
We overload notation for addition (\(+\)) and multiplication (\(\cdot\))
in algebraic structures when possible to avoid clutter.
Similarly, the additive identity (\(0\)) and multiplicative identity (\(1\))
are also overloaded when possible.

\begin{definition}[Monoid]
  A monoid is an algebraic structure \(\parens{\mbb{K}, \cdot, 1}\) where:
  
  \begin{itemize}
    \item
      \(\mbb{K}\) is closed under monoid multiplication
      \(\type{\cdot}{\mbb{K} \times \mbb{K}}{\mbb{K}}\).

    \item
      \(1\) is the multiplicative identity.

  \end{itemize}

  When possible, we elide the \(\cdot\) in monoid multiplication to write
  \(ab\) instead of \(a \cdot b\).
  When multiplication \(\cdot\) is commutative,
  the system is known as a commutative monoid.
\end{definition}

\begin{example}
  Consider a finite alphabet \(\Sigma\),
  then the system \(\parens{\Sigma^{\star}, \cdot, \varepsilon}\)
  forms a monoid,
  where monoid multiplication \(\cdot\)
  is string concatenation and \(\varepsilon\) is the empty string.
\end{example}


\begin{definition}[Semiring]
  A semiring \(\parens{\mbb{K}, +, \cdot, 0, 1}\) is a system where:

  \begin{itemize}
    \item
      Semiring addition is a commutative monoid
      \(\parens{\mbb{K}, +, 0}\).

    \item
      Semiring multiplication
      \(\parens{\mbb{K}, \cdot, 1}\) is a monoid.

    \item
      \(0\) annihilates semiring multiplication.
  \end{itemize}

\end{definition}

\begin{example}
  For a finite alphabet \(\Sigma\),
  the system
  \(\parens{\powset{\Sigma^\star}, \cup, \cdot, \emptyset, \varepsilon}\)
  forms a semiring.
  Here semiring addition is set union,
  and semiring multiplication is defined as:
  \begin{align*}
    A \cdot B
      = \braces{a \cdot b \st a \in A, b \in B}
  \end{align*}
  for all \(A, B \in \powset{\Sigma^\star}\).
  We take
  the additive identity to be the empty set \(\emptyset\),
  and the multiplicative identity as the empty string \(\varepsilon\).
\end{example}

A weighted finite state transducer (WFST) is a very
general transition system defined
using a semiring to specify transition weights.


\begin{definition}[Weighted Finite State Transducer]
  A weighted finite state transducer over a semiring \(\mbb{K}\) is
  a system
  \(\parens{\Sigma_I, \Sigma_O, Q, I, F, \Delta, \lambda, \rho}\) where:

  \begin{itemize}
    \item
      \(\Sigma_I\) is a finite input alphabet.

    \item
      \(\Sigma_O\) is a finite output alphabet.

    \item
      \(Q\) is a finite set of states.

    \item
      \(I \subseteq Q\) is the set of starting states.

    \item
      \(F \subseteq Q\) is the set of final states.

    \item
      \(\Delta \subseteq
          Q \times
          \parens{\Sigma_I \cup \braces{\varepsilon}} \times
          \mbb{K} \times
          \parens{\Sigma_O \cup \braces{\varepsilon}} \times
          Q \)
      is the transition function weighted by \(\mbb{K}\).
        
    \item
      \(\type{\lambda}{I}{\mbb{K}}\) is the initial state weight function.

    \item
      \(\type{\rho}{F}{\mbb{K}}\) is the final state weight function.

    \end{itemize}

    Note that the transition function \(\Delta\) is defined to
    permit non-deterministic behavior by default.
\end{definition}

It should be noted that \(\Sigma_I\) and \(\Sigma_O\) can be
seen as the generators of the free monoids
\(\Sigma_I ^\star\) and \(\Sigma_O ^\star\),
which represents the set of all strings over
\(\Sigma_I\) and \(\Sigma_O\) respectively.

Although we are not yet interested in the full generality that
a WFST offers, it is still nice to see what is available to us
in terms of abstraction.

A special case of WFSTs that we are interested in
are non-deterministic finite automata,
whose specification in terms of WSFTs
is by Cortes~\cite{cortes2004rational}.
We introduce a simplified structure.

\begin{definition}[Non-deterministic Finite Automata]
  A non-deterministic finite automata is a system
  \(\parens{\Sigma, Q, \Delta, I, F}\) where:

  \begin{enumerate}
    \item
      \(\Sigma\) is a finite alphabet.

    \item
      \(Q\) is a finite set of states.

    \item
      \(\Delta \subseteq Q \times \Sigma \times Q\)
      is the transition function.

    \item
      \(I \subseteq Q\) is the set of initial states.

    \item
      \(F \subseteq Q\) is the set of final states.

  \end{enumerate}
\end{definition}

We do not permit \(\varepsilon\)-transitions in our definition,
which can be eliminated anyways~\cite{savage1998models}.



\subsection{Reproducing Kernel Hilbert Spaces}

Inner products spaces are nice because they allow us to measure projection.
Additionally, an inner product induces a norm, from which a metric,
and thus topology, can also be created.

\begin{definition}[Hilbert Space]
  A Hilbert space is a inner product space that is complete with respect
  to the metric induced by its inner product.
\end{definition}

If \(\mcal{H}\) is a Hilbert space,
write
\(\type{\angles{\cdot, \cdot}_{\mcal{H}}}{\mcal{H} \times \mcal{H}}{\R}\)
to denote its inner product.
We drop the subscript when context is clear,
and remark that another option is for complex-valued inner products.

A special type of Hilbert spaces are known as
reproducing kernel Hilbert spaces~\cite{berlinet2011reproducing}.
In short,
these are Hilbert spaces with a special function known
as the reproducing kernel.


\begin{definition}[Reproducing Kernel]
  Let \(\mcal{H}\) be a Hilbert space.
  A reproducing kernel is a function
  \(\type{k}{\mcal{H} \times \mcal{H}}{\R}\)
  that satisfies the property
  \begin{align*}
    f\parens{x} = \angles{f, k\parens{x, \cdot}}_H
  \end{align*}
  for all \(f \in \mcal{H}\).
\end{definition}

A reproducing kernel Hilbert space can be induced by the existence
of a kernel function (not to be confused with the reproducing kernel).
This type of kernel function can be defined over general sets,
and provides a way of taking inner products by embedding them
into a Hilbert space.

\begin{definition}[Kernel]
  Let \(\mcal{X}\) be a set.
  A function \(\type{k}{\mcal{X} \times \mcal{X}}{\R}\) if:

  \begin{enumerate}
    \item
      \(k\parens{x, y} = k\parens{y, x}\) for all \(x, y \in \mcal{X}\).
      This is known as symmetric.

    \item
      If for all \(x_1, x_2, \ldots, x_n \in \mcal{X}\)
      the Gram matrix \(K\)
      defined by:
      \begin{align*}
        K_{i, j} = k\parens{x_i, x_j}
      \end{align*}
      is positive semi-definite.

  \end{enumerate}

\end{definition}

Given a set \(\mcal{X}\) and a kernel
\(\type{k}{\mcal{X} \times \mcal{X}}{\R}\),
one can generate a Hilbert space.
Define a ``feature map'' \(\type{\phi}{\mcal{X}}{\R^{\mcal{X}}}\)
as follows:
\begin{align*}
  \phi\parens{x} = k\parens{x, \cdot}
\end{align*}
Then \(\phi\) maps each element of \(\mcal{X}\)
into a Hilbert space \(\mcal{H}\) consisting of the closure of
the span of functions \(\type{f}{\mcal{X}}{\R}\):
\begin{align*}
  \mcal{H} =
    \overline{\vspan\braces{\type{f}{\mcal{X}}{\R}}}
    = \overline{
        \braces{
          \sum_{i = 1}^{n} a_i k\parens{\cdot, x_i}
            \st n \in \N, x_i \in \mcal{X}, a_i \in \R}}
\end{align*}
Consider two functions \(f, g \in \mcal{H}\) which would have form:
\begin{align*}
  f\parens{x} = \sum_{i \in I} a_i k\parens{x, u_i}
  \qquad \qquad
  g\parens{x} = \sum_{j \in J} b_j k\parens{x, v_j}
\end{align*}
where each \(u_i, v_j \in \mcal{X}\).
Note that the summation may be countably infinite over the index sets \(I\)
and \(J\), since Hilbert spaces are complete with
respect to the norm induced by the inner product.
The inner product between \(f\) and \(g\) is then defined as:
\begin{align*}
  \angles{f, g}
    = \angles{\sum_{i \in I} a_i k\parens{\cdot, u_i},
              \sum_{j \in J} b_i k\parens{\cdot, v_j}}
    = \sum_{i \in I} \sum_{j \in J} a_i b_j k\parens{u_i, v_j}
\end{align*}
This is further described and proven in previous
work~\cite{cortes2004rational, vishwanathan2010graph}.



\subsection{Tensor Products}
Tensor algebra are treated in more detail
in other texts~\cite{itskov2007tensor},
and multiple generalizations exist.
In this sketch we are interested in a very specialized slice of
tensor algebra.
Note that we use the term linear space instead of vector space,
since much of linear algebra, including tensor algebra,
does not necessarily have to be defined over fields.

\begin{definition}[Tensor Product of Linear Spaces]
  Let \(V\) be a linear space with countable basis
  \(\braces{v_1, v_2, \ldots}\),
  and \(W\) be a linear space with countable basis
  \(\braces{w_1, w_2, \ldots}\).
  The tensor product \(V \otimes W\) is the linear space
  spanned by the countable basis \(\braces{v_i \otimes w_j}\).
\end{definition}

Note that we treat \(v_i \otimes w_j\)
as symbols without special meaning attributed to them.
For instance, if \(V\) has basis \(\braces{v_1, v_2}\)
and \(W\) has basis \(\braces{w_1, w_2, w_3}\),
then \(V \otimes W\) has basis:
\begin{align*}
  \braces{v_1 \otimes w_1, v_1 \otimes w_2, v_1 \otimes w_3,
          v_2 \otimes w_1, v_2 \otimes w_2, v_2 \otimes w_3}
\end{align*}
Note that the above basis written in this manner can be considered
an ordered basis of \(V \otimes W\).

Additionally, if \(\type{S}{V}{W}\) and \(\type{T}{X}{Y}\)
are linear transformations,
then the tensor product would have type:
\begin{align*}
  \type{S \otimes T}{V \otimes X}{W \otimes Y}
\end{align*}

Finite-dimensional linear transformations are often envisioned
as matrices that act on column vectors by left multiplication.
For two matrices \(A_{m \times n}\) and \(B_{p \times q}\)
that represent linear transforms
\(\type{A}{\R^n}{\R^m}\) and \(\type{B}{\R^q}{\R^p}\) respectively,
the Kronecker product is a way to lift the linear
transform represented by \(A\) and \(B\) into the tensor product space.
We write this in terms of block matrices as follows:
\begin{align*}
  {A \otimes B}
    = \begin{bmatrix}
        A_{1, 1} B & \ldots & A_{1, n} B \\
        \vdots & \ddots & \vdots \\
        A_{m, 1} B & \ldots & A_{m, n} B
      \end{bmatrix}
\end{align*}
This is then a linear transformation with respect to the ordered basis
\(\R^n \otimes \R^q\) and \(\R^m \otimes \R^p\).

The Kronecker product on matrices is also used as a tensor product over
adjacency matrices of graphs to define the notion of a product graph.



\section{Embedding Automata}


\subsection{Simple Graphs}

We first consider kernel methods over graphs, which is the main focus
of Vishwanathan~\cite{vishwanathan2010graph}.
The main idea is that the space of simple graphs can be treated
using kernel methods described above.
Here, simple graphs are taken to mean (un)-directed graphs
that are representable by \(0 / 1\) adjacency matrices.

Let \(G = \parens{V_G, E_G}\) and
\(H = \parens{V_H, E_H}\) be two finite simple graphs.
Here \(V_G\) is the vertex set of \(G\), and \(E_H \subseteq V_G \times V_G\)
is the edge set of \(G\), with similar notation for \(H\).
Use the same letters to represent their adjacency matrices in an abuse
of notation.
Then a kernel on this space of graph adjacency matrices can be
defined as follows:
\begin{align*}
  k\parens{G, H}
    = \sum_{k = 0}^{\infty} \mu\parens{k} q^T \parens{G \otimes H}^k p
\end{align*}
where \(\type{\mu}{\N}{\R}\) is a function over \(\N\)
to help ensure that the summation converges,
\(q\) and \(p\) are fixed and of the appropriate dimensions
intended to mean something along the lines of final
and initial weights with respect to a basis on \(V_G \otimes H_G\).

Note that adjacency matrices may either act on column vectors by
left multiplication or row vectors by right multiplication,
whichever is convenient.

Suppose that \(G\) and \(H\) are known,
then a normalized version of the kernel can be defined as:
\begin{align*}
  k\parens{G, H}
    = \frac{1}{\abs{V_G}^2 \abs{V_H}^2}
      \sum_{k = 0}^{\infty} \mu\parens{k} q^T \parens{G \otimes H}^k p
\end{align*}
Since \(G\) and \(H\) represent adjacency matrices respectively,
if \(q\) and \(p\) take on values in \(\brackets{0, 1}\),
then each iteration of the summation does not exceed the dimension
of \(G \otimes H\),
which is \(\abs{V_G}^2 \abs{V_H}^2\).
In addition, if \(\mu\) is a probability distribution on \(\N\), then:
\begin{align*}
  0 \leq k\parens{G, H} \leq 1
\end{align*}
for all simple graphs \(G\) and \(H\).





\subsection{Non-deterministic Finite Automata}
A similar trick can be used for NFAs.
However a few considerations have to be made when representing NFAs
as matrices.
In particular, because semiring multiplication is not commutative,
we must be extra careful in our construction.

Consider an NFA \(\parens{\Sigma, Q, \Delta, S, F}\).
We want to represent this as a matrix \(A_{\abs{Q} \times \abs{Q}}\)
where \(A_{i, j}\) denotes the transition from state \(q_i\)
to state \(q_j\).
We arbitrarily choose to read strings left to right, so this implies that
\(A\) should act by right multiplication on row vectors.

Additionally, we must account for the fact that edges in an NFA
may be weighted by multiple elements (or strings, in a more general setting)
from the alphabet.
This suggests that each entry of \(A\) should instead be a set:
\begin{align*}
  A_{i, j} =
    \braces{\sigma \st \parens{q_i, \sigma, q_j} \in \Delta}
\end{align*}
In fact, it is possible to view each entry of the \(A\) matrix
as a member of the
\(\parens{\powset{\Sigma^\star}, \cup, \cdot, \emptyset, \varepsilon}\)
semiring, which was discussed earlier.
Note that technically this would technically permit each transition
to consume a string of letters rather than a single one,
but we could just be careful in our construction to avoid
these cases that would make analysis efforts more difficult.
All together, \(A\) would then be typed as follows:
\begin{align*}
  \type{A}{\parens{\Sigma^\star}^{\abs{Q}}}{\parens{\Sigma^\star}^{\abs{Q}}}
\end{align*}
where the semiring operations and identities are implicit.
For each \(1 \leq i, j \leq \abs{Q}\),
the entry \(A_{i, j}\) is still defined as before,
and this \(A\) matrix acts on row vectors by right multiplication.

Additionally, take \(\alpha\) to be a \(\abs{Q}\)-sized vector
representing the initial states, where:
\begin{align*}
  \alpha_i =
    \begin{cases}
      \braces{\varepsilon} &\quad q_i \in I \\
      \emptyset &\quad \ow
    \end{cases}
\end{align*}
Define \(\beta\) to be the vector representing the final states
in a similar manner.

We will sometimes overload notation to have \(A\) mean both the transition
matrix and the NFA that it represents.

Consider two NFAs:
\begin{align*}
  A = \parens{\Sigma_A, Q_A, \Delta_A, I_A, F_A}
  \qquad \qquad
  B = \parens{\Sigma_B, Q_B, \Delta_B, I_B, F_B}
\end{align*}
A normalized kernel can then be defined as:
\begin{align*}
  k\parens{A, B}
    = \frac{1}{\abs{\Sigma_A} \abs{Q_A}^2 \abs{\Sigma_B} \abs{Q_B}^2}
        \sum_{k = 0}^{\infty}
          \mu\parens{k}
          \parens{\alpha_A \otimes \alpha_B}^T
            \parens{A \otimes B}^k
            \parens{\beta_A \otimes \beta_B}
\end{align*}




\section{Bounding Inner Products}

\red{What do lower bounds on inner products mean??}



\printbibliography


\end{document}


