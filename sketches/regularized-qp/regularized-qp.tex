\documentclass[12pt]{article}

% Packages
\usepackage[margin=5em]{geometry} % 1 cm = 2.84528 em
\usepackage[backend=bibtex]{biblatex}
\bibliography{sources}
% \nocite{*}

\usepackage{lipsum}

% Paragraphs
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% Includes
\input{axlib.tex}

% Author
\title{Regularized Equality Constrained Quadratic Optimization}
\author{Anton Xue and Nikolai Matni}
\date{\today}
\date{}

% Document
\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
We look at regularized approximations
of equality-constrained quadratic programming.
In particular, how much does the optimal solution deviate
when a regularizer is introduced?


\section{Background}
Stephen Boyd and Lieven Vandenberghe~\cite{boyd2004convex}.

\subsection{Equality Constrained Quadratic Programming}

A quadratic program with equality constraints~\cite{boyd2004convex} is
\begin{align}
  \text{minimize} &\quad \frac{1}{2} x^\top Q x
    \label{eqn:qp} \\
  \text{subject to} &\quad Ax = b
\end{align}
with variable in \(x \in \mbb{R}^n\),
where \(Q \succeq 0\) and the constraint \(A \in \mbb{R}^{m \times n}\) is,
for our purposes, fat and full rank.

The \(\ell^2\) regularized version for \(\lambda > 0\) looks like
\begin{align}
  \text{minimize} &\quad \frac{1}{2} x^\top Q x + \lambda x^\top x
    \label{eqn:reg-qp} \\
  \text{subject to} &\quad Ax = b
\end{align}

\subsection{Karush-Kuhn-Tucker Conditions}
The Lagrangian for \eqref{eqn:qp} is of form
\begin{align*}
  L(x, p) = \frac{1}{2} x^\top Q x + p^\top (Ax - b)
\end{align*}
for which the
optimality conditions~\cite{boyd2004convex} are
\begin{align*}
  Q x^\star + A^\top p^\star = b,
    \qquad Ax^\star = b
\end{align*}
or more compcatctly expressed:
\begin{align}
  \begin{bmatrix} Q & A^\top \\ A & 0 \end{bmatrix}
  \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} 0 \\ b \end{bmatrix}
    \label{eqn:kkt-qp}
\end{align}

On the other hand the Lagrangian of the
regularized problem \eqref{eqn:reg-qp} is
\begin{align*}
  L_\lambda (x, p) = \frac{1}{2} x^\top (Q + \lambda I) x + p^\top (Ax - b)
\end{align*}
which has the KKT conditions
\begin{align}
  \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
  \begin{bmatrix} x_\lambda ^\star \\ p_\lambda ^\star \end{bmatrix}
    = \begin{bmatrix} 0 \\ b \end{bmatrix}
    \label{eqn:kkt-reg-qp}
\end{align}

\subsection{Matrix Inversion}
Consider a symmetric matrix
\begin{align*}
  Q = \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}
\end{align*}
when \(A \succ 0\), define
the Schur complement \(S = C - B^\top A^{-1} B\).
Then
\begin{align*}
  \begin{bmatrix} A & B \\ B^\top & C \end{bmatrix}^{-1}
    \begin{bmatrix}
      A^{-1} + A^{-1} B S^{-1} B^\top A^{-1} & - A^{-1} B S^{-1} \\
      - S^{-1} B^\top A^{-1} & S^{-1}
    \end{bmatrix}
\end{align*}


\section{Bounding Norms}
Noting that by linearity
\begin{align*}
  \begin{bmatrix} Q & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  + \begin{bmatrix} \lambda I & 0 \\ 0 & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix} x^\star \\ p^\star \end{bmatrix}
  = \begin{bmatrix} \lambda x^\star \\ b \end{bmatrix}
\end{align*}
and so through subtracting equations,
\begin{align*}
  \begin{bmatrix} Q + \lambda I & A^\top \\ A & 0 \end{bmatrix}
    \begin{bmatrix}
      x^\star - x_{\lambda} ^\star \\ p^\star - p_\lambda ^\star
    \end{bmatrix}
    &= \begin{bmatrix} \lambda x^\star \\ 0 \end{bmatrix}
\end{align*}
Given our assumptions on \(Q + \lambda I \succ 0\) and \(A\) is full rank.
For ease of notation, define \(\Lambda = Q + \lambda I\).
The Schur complement is then
\(S = - A \Lambda^{-1} A^\top\), and
\begin{align*}
  \begin{bmatrix}
    x^\star - x_\lambda ^\star \\ p^\star - p_\lambda ^\star \end{bmatrix}
  = \begin{bmatrix}
      \Lambda^{-1} + \Lambda^{-1} A^\top S^{-1} A \Lambda^{-1}
        & - \Lambda^{-1} A^\top S^{-1} \\
      - S^{-1} A \Lambda^{-1} & S^{-1}
    \end{bmatrix}
    \begin{bmatrix}
      \lambda x^\star \\ 0
    \end{bmatrix}
\end{align*}
In other words:
\begin{align*}
  x^\star - x_\lambda ^\star
    = \parens{\Lambda^{-1} -
      \Lambda^{-1} A^\top (A \Lambda^{-1} A^\top)^{-1} A \Lambda^{-1}}
      \lambda x^\star
\end{align*}
To simplify notation slightly, use \(\Gamma = \Lambda^{-1}\)
because it looks like an upside-down \(L\).
Then
\begin{align*}
  x^\star - x_\lambda ^\star
    = \parens{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma)}
      \lambda x^\star
\end{align*}

\begin{theorem}
  Without loss of generality, assume that
  \begin{align*}
    Q = \begin{bmatrix} q_1 & & \\ & \ddots & \\ & & q_n \end{bmatrix},
      \qquad q_1 \geq \cdots \geq q_n > 0.
  \end{align*}
  Then for \(\lambda \leq q_n\),
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \parens{\frac{\lambda}{q_n + \lambda}
              - \frac{\lambda}{q_1 + \lambda}} \norm{x^\star}
  \end{align*}
\end{theorem}
\begin{proof}
  First note that
  \begin{align*}
    \Lambda =
      \begin{bmatrix}
        q_1 + \lambda & & \\ & \ddots & \\ & & q_n + \lambda
      \end{bmatrix},
      \qquad
      \Gamma = \Lambda^{-1}
        = \begin{bmatrix}
            \frac{1}{q_1 + \lambda} & & \\
            & \ddots & \\
            & & \frac{1}{q_n + \lambda}
          \end{bmatrix}
  \end{align*}
  We seek the bound the RHS of the inequality
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \norm{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma}
          \cdot \lambda \norm{x^\star}
  \end{align*}
  The primary challenge here is
  correctly \textit{lower bounding} the
  \(\Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma\) term.
  For this we \textit{upper bound} the \(A \Gamma A^\top\) term.
  Letting
  \begin{align*}
    A = U \Sigma V^\top
      = U \begin{bmatrix} \Sigma_1 & 0 \end{bmatrix}
          \begin{bmatrix} V_1 ^\top \\ V_2 ^\top \end{bmatrix},
        \qquad \Sigma_1 =
        \begin{bmatrix} \sigma_1 & & \\ & \ddots & \\ & & \sigma_m \end{bmatrix}
  \end{align*}
  be an SVD of \(A\); an upper bound of \(A \Gamma A^\top\) is
  \begin{align*}
    A \Gamma A^\top
      = U \Sigma_1 V_1 ^\top
          \begin{bmatrix}
            \frac{1}{q_1 + \lambda} & & \\
            & \ddots & \\
            & & \frac{1}{q_n + \lambda}
          \end{bmatrix}
          V_1 \Sigma_1 U^\top
      \preceq \frac{1}{q_n + \lambda} U \Sigma_1 ^2 U^\top
  \end{align*}
  Consequently, a lower bound for
  \(\Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma\) is
  \begin{align*}
    \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma
      \succeq
        \Gamma V_1 \Sigma_1 U^\top
          \parens{\frac{1}{q_n + \lambda} U \Sigma_1 ^2 U^\top}^{-1}
          U \Sigma_1 V_1 ^\top \Gamma
      = \parens{q_n + \lambda}
          \Gamma V_1 V_1 ^\top \Gamma
      \succeq \frac{q_n + \lambda}{q_1 + \lambda} \Gamma
  \end{align*}
  Then using this,
  \begin{align*}
    \Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma
      \preceq \Gamma - \frac{q_n + \lambda}{q_1 + \lambda} \Gamma
      \preceq \parens{\frac{1}{q_n + \lambda} - \frac{1}{q_1 + \lambda}} I
  \end{align*}
  for which we derive the desired bound
  \begin{align*}
    \norm{x^\star - x_\lambda ^\star}
      \leq \norm{\Gamma - \Gamma A^\top (A \Gamma A^\top)^{-1} A \Gamma}
          \cdot \lambda \norm{x^\star}
      \leq \parens{\frac{\lambda}{q_n + \lambda}
                - \frac{\lambda}{q_1 + \lambda}}
              \norm{x^\star}
  \end{align*}

\end{proof}


\printbibliography

\end{document}

